{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Kafka Basics - Stock Market Data Streaming\n",
    "\n",
    "## ğŸ¯ Objectives\n",
    "- Understand Kafka fundamentals: Topics, Partitions, Producers, Consumers\n",
    "- Learn message serialization with JSON and custom serializers\n",
    "- Practice basic producer/consumer patterns\n",
    "- Explore Kafka UI for monitoring\n",
    "\n",
    "## ğŸ“‹ Prerequisites\n",
    "- Kafka cluster running (`docker compose up -d`)\n",
    "- Python dependencies installed (`./setup_kafka_lab.sh`)\n",
    "- Basic understanding of message queues\n",
    "\n",
    "## ğŸ—ï¸ Architecture Overview\n",
    "```\n",
    "Stock Data Generator â†’ Kafka Producer â†’ Stock Data Topic\n",
    "                                                      â†“\n",
    "                                              Multiple Consumers\n",
    "                                                      â†“\n",
    "                                            Analytics, Alerts, Storage\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: confluent-kafka in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (2.12.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (6.5.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (from plotly) (2.14.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Dependencies installed and imported successfully!\n"
     ]
    }
   ],
   "source": [
    "%pip install kafka-python confluent-kafka pandas matplotlib seaborn plotly\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from kafka.errors import KafkaError\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List\n",
    "\n",
    "print(\"Dependencies installed and imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured for 10 stock symbols\n",
      "Kafka Bootstrap Servers: localhost:9092\n",
      "Topic Name: stock-data\n"
     ]
    }
   ],
   "source": [
    "# Kafka Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "TOPIC_NAME = 'stock-data'\n",
    "\n",
    "# Stock symbols for our lab\n",
    "STOCK_SYMBOLS = [\n",
    "    \"AAPL\", \"GOOGL\", \"MSFT\", \"TSLA\", \"AMZN\", \n",
    "    \"META\", \"NVDA\", \"NFLX\", \"ADBE\", \"CRM\"\n",
    "]\n",
    "\n",
    "# Base prices for realistic data generation\n",
    "BASE_PRICES = {\n",
    "    \"AAPL\": 150.0, \"GOOGL\": 2800.0, \"MSFT\": 350.0, \"TSLA\": 250.0, \"AMZN\": 3200.0,\n",
    "    \"META\": 300.0, \"NVDA\": 450.0, \"NFLX\": 400.0, \"ADBE\": 500.0, \"CRM\": 200.0\n",
    "}\n",
    "\n",
    "print(f\"Configured for {len(STOCK_SYMBOLS)} stock symbols\")\n",
    "print(f\"Kafka Bootstrap Servers: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
    "print(f\"Topic Name: {TOPIC_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Stock Data Generator initialized\n",
      "ğŸ“Š Available symbols: AAPL, GOOGL, MSFT, TSLA, AMZN, META, NVDA, NFLX, ADBE, CRM\n"
     ]
    }
   ],
   "source": [
    "# Stock Data Generator\n",
    "class StockDataGenerator:\n",
    "    \"\"\"Generate realistic OHLCV stock data\"\"\"\n",
    "    \n",
    "    def __init__(self, symbols: List[str], base_prices: Dict[str, float]):\n",
    "        self.symbols = symbols\n",
    "        self.base_prices = base_prices\n",
    "        self.current_prices = base_prices.copy()\n",
    "    \n",
    "    def generate_ohlcv(self, symbol: str) -> Dict:\n",
    "        \"\"\"Generate OHLCV data for a stock symbol\"\"\"\n",
    "        current_price = self.current_prices[symbol]\n",
    "        price_change = random.uniform(-0.01, 0.01)\n",
    "        new_price = current_price * (1 + price_change)\n",
    "        \n",
    "        open_price = round(new_price * random.uniform(0.999, 1.001), 2)\n",
    "        close_price = round(new_price * random.uniform(0.999, 1.001), 2)\n",
    "        high_price = round(max(open_price, close_price) * random.uniform(1.001, 1.003), 2)\n",
    "        low_price = round(min(open_price, close_price) * random.uniform(0.997, 0.999), 2)\n",
    "        \n",
    "        base_volume = random.randint(100000, 1000000)\n",
    "        volume = base_volume + random.randint(-100000, 100000)\n",
    "        \n",
    "        self.current_prices[symbol] = close_price\n",
    "        \n",
    "        return {\n",
    "            \"symbol\": symbol,\n",
    "            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"open\": open_price,\n",
    "            \"high\": high_price,\n",
    "            \"low\": low_price,\n",
    "            \"close\": close_price,\n",
    "            \"volume\": volume,\n",
    "            \"exchange\": \"NASDAQ\"\n",
    "        }\n",
    "\n",
    "# Initialize data generator\n",
    "data_generator = StockDataGenerator(STOCK_SYMBOLS, BASE_PRICES)\n",
    "print(\"âœ… Stock Data Generator initialized\")\n",
    "print(f\"ğŸ“Š Available symbols: {', '.join(STOCK_SYMBOLS)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Understanding Kafka Fundamentals\n",
    "\n",
    "### ğŸ¯ **Learning Objectives:**\n",
    "- Understand Kafka core concepts: Topics, Partitions, Producers, Consumers\n",
    "- Learn about message serialization and deserialization\n",
    "- Practice basic producer/consumer patterns\n",
    "- Explore Kafka cluster monitoring\n",
    "\n",
    "### ğŸ“š **Key Concepts:**\n",
    "1. **Topic**: A category or feed name to which messages are published\n",
    "2. **Partition**: Topics are split into partitions for scalability\n",
    "3. **Producer**: Application that sends messages to Kafka topics\n",
    "4. **Consumer**: Application that reads messages from Kafka topics\n",
    "5. **Broker**: Kafka server that stores and serves messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Creating Kafka Producer...\n",
      "âœ… Kafka Producer created successfully!\n",
      "ğŸ“¡ Bootstrap servers: localhost:9092\n",
      "ğŸ“ Topic: stock-data\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Create Kafka Producer\n",
    "print(\"ğŸ”§ Creating Kafka Producer...\")\n",
    "\n",
    "try:\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "        key_serializer=lambda k: k.encode('utf-8') if k else None,\n",
    "        acks='all', #Ensure all replicas acknowledge\n",
    "        retries=3,\n",
    "        batch_size=16384,\n",
    "        linger_ms=10,\n",
    "        compression_type='gzip'\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Kafka Producer created successfully!\")\n",
    "    print(f\"ğŸ“¡ Bootstrap servers: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
    "    print(f\"ğŸ“ Topic: {TOPIC_NAME}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to create producer: {e}\")\n",
    "    print(\"ğŸ’¡ Make sure Kafka cluster is running: docker compose up -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Generating and sending stock data...\n",
      "ğŸ“Š Sending 15 stock data messages...\n",
      "ğŸ“Š Sent META: $301.03 -> Partition 0\n",
      "ğŸ“Š Sent AAPL: $150.7 -> Partition 0\n",
      "ğŸ“Š Sent META: $303.24 -> Partition 0\n",
      "ğŸ“Š Sent AMZN: $3226.32 -> Partition 0\n",
      "ğŸ“Š Sent CRM: $201.7 -> Partition 0\n",
      "ğŸ“Š Sent AMZN: $3241.27 -> Partition 0\n",
      "ğŸ“Š Sent AAPL: $151.73 -> Partition 0\n",
      "ğŸ“Š Sent NVDA: $450.01 -> Partition 0\n",
      "ğŸ“Š Sent NVDA: $451.08 -> Partition 0\n",
      "ğŸ“Š Sent NVDA: $448.68 -> Partition 0\n",
      "ğŸ“Š Sent NVDA: $448.27 -> Partition 0\n",
      "ğŸ“Š Sent TSLA: $251.83 -> Partition 0\n",
      "ğŸ“Š Sent AAPL: $151.2 -> Partition 0\n",
      "ğŸ“Š Sent CRM: $202.25 -> Partition 0\n",
      "ğŸ“Š Sent GOOGL: $2823.16 -> Partition 0\n",
      "âœ… Successfully sent 15 messages to topic 'stock-data'\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: Generate and Send Stock Data\n",
    "print(\"ğŸ“ˆ Generating and sending stock data...\")\n",
    "\n",
    "def send_stock_data(num_messages: int = 10):\n",
    "    \"\"\"Send stock data to Kafka topic\"\"\"\n",
    "    print(f\"ğŸ“Š Sending {num_messages} stock data messages...\")\n",
    "    \n",
    "    for i in range(num_messages):\n",
    "        symbol = random.choice(STOCK_SYMBOLS)\n",
    "        ohlcv_data = data_generator.generate_ohlcv(symbol)\n",
    "        \n",
    "        future = producer.send(TOPIC_NAME, key=symbol, value=ohlcv_data)\n",
    "        record_metadata = future.get(timeout=10)\n",
    "        \n",
    "        print(f\"ğŸ“Š Sent {symbol}: ${ohlcv_data['close']} -> Partition {record_metadata.partition}\")\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    producer.flush()\n",
    "    print(f\"âœ… Successfully sent {num_messages} messages to topic '{TOPIC_NAME}'\")\n",
    "\n",
    "# Send some test data\n",
    "send_stock_data(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Creating Kafka Consumer...\n",
      "âœ… Kafka Consumer created successfully!\n",
      "ğŸ‘¥ Consumer Group: stock-analytics-group\n",
      "ğŸ“ Topic: stock-data\n",
      "ğŸ”„ Auto Offset Reset: earliest\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: Create Kafka Consumer\n",
    "print(\"ğŸ”§ Creating Kafka Consumer...\")\n",
    "\n",
    "try:\n",
    "    consumer = KafkaConsumer(\n",
    "        TOPIC_NAME,\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "        group_id='stock-analytics-group',\n",
    "        auto_offset_reset='earliest',\n",
    "        enable_auto_commit=True,\n",
    "        value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
    "        key_deserializer=lambda x: x.decode('utf-8') if x else None,\n",
    "        consumer_timeout_ms=5000\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Kafka Consumer created successfully!\")\n",
    "    print(f\"ğŸ‘¥ Consumer Group: stock-analytics-group\")\n",
    "    print(f\"ğŸ“ Topic: {TOPIC_NAME}\")\n",
    "    print(f\"ğŸ”„ Auto Offset Reset: earliest\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to create consumer: {e}\")\n",
    "    print(\"ğŸ’¡ Make sure Kafka cluster is running: docker compose up -d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Consuming messages from Kafka...\n",
      "ğŸš€ Starting to consume up to 10 messages...\n",
      "âš ï¸ Finished consuming: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n",
      "ğŸ“ˆ Consumption Summary:\n",
      "   Total messages consumed: 0\n",
      "   Partition distribution: {}\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: Consume Messages\n",
    "print(\"ğŸ”„ Consuming messages from Kafka...\")\n",
    "\n",
    "def consume_messages(max_messages: int = 10):\n",
    "    \"\"\"Consume messages from Kafka topic\"\"\"\n",
    "    messages_consumed = 0\n",
    "    partition_counts = {}\n",
    "    \n",
    "    print(f\"ğŸš€ Starting to consume up to {max_messages} messages...\")\n",
    "    \n",
    "    try:\n",
    "        for message in consumer:\n",
    "            if messages_consumed >= max_messages:\n",
    "                break\n",
    "            \n",
    "            partition = message.partition\n",
    "            partition_counts[partition] = partition_counts.get(partition, 0) + 1\n",
    "            \n",
    "            key = message.key\n",
    "            value = message.value\n",
    "            offset = message.offset\n",
    "            timestamp = message.timestamp\n",
    "            \n",
    "            print(f\"ğŸ“Š Message {messages_consumed + 1}:\")\n",
    "            print(f\"   Key: {key}\")\n",
    "            print(f\"   Symbol: {value['symbol']}\")\n",
    "            print(f\"   Price: ${value['close']}\")\n",
    "            print(f\"   Volume: {value['volume']:,}\")\n",
    "            print(f\"   Partition: {partition}\")\n",
    "            print(f\"   Offset: {offset}\")\n",
    "            print(f\"   Timestamp: {datetime.fromtimestamp(timestamp/1000)}\")\n",
    "            print()\n",
    "            \n",
    "            messages_consumed += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Finished consuming: {e}\")\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Consumption Summary:\")\n",
    "    print(f\"   Total messages consumed: {messages_consumed}\")\n",
    "    print(f\"   Partition distribution: {partition_counts}\")\n",
    "    \n",
    "    return messages_consumed, partition_counts\n",
    "\n",
    "# Consume messages\n",
    "messages_count, partition_distribution = consume_messages(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Demonstrating custom serialization...\n",
      "âœ… Custom serializers/deserializers created!\n",
      "\n",
      "ğŸ§ª Testing custom serialization...\n",
      "ğŸ“Š Sent complex data to partition 0\n",
      "\n",
      "ğŸ”„ Consuming complex data...\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '{'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Consume and verify\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ”„ Consuming complex data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m custom_consumer:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“Š Received complex data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Symbol: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages\\kafka\\consumer\\group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages\\kafka\\consumer\\group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages\\kafka\\consumer\\group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1115\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m-> 1116\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[0;32m   1118\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[0;32m   1122\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages\\kafka\\consumer\\group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    653\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[0;32m    657\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[1;32mc:\\Users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages\\kafka\\consumer\\group.py:708\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mneed_rejoin():\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[1;32m--> 708\u001b[0m records, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetched_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[1;32mc:\\Users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages\\kafka\\consumer\\fetcher.py:344\u001b[0m, in \u001b[0;36mFetcher.fetched_records\u001b[1;34m(self, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    343\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_completed_fetches\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_partition_records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_fetched_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m     records_remaining \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append(drained,\n\u001b[0;32m    347\u001b[0m                                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_partition_records,\n\u001b[0;32m    348\u001b[0m                                       records_remaining,\n\u001b[0;32m    349\u001b[0m                                       update_offsets)\n",
      "File \u001b[1;32mc:\\Users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages\\kafka\\consumer\\fetcher.py:818\u001b[0m, in \u001b[0;36mFetcher._parse_fetched_data\u001b[1;34m(self, completed_fetch)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m records\u001b[38;5;241m.\u001b[39mhas_next():\n\u001b[0;32m    815\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding fetched record for partition \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m offset \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m to buffered record list\u001b[39m\u001b[38;5;124m\"\u001b[39m, tp,\n\u001b[0;32m    817\u001b[0m               position)\n\u001b[1;32m--> 818\u001b[0m     unpacked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unpack_message_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    819\u001b[0m     parsed_records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPartitionRecords(fetch_offset, tp, unpacked)\n\u001b[0;32m    820\u001b[0m     last_offset \u001b[38;5;241m=\u001b[39m unpacked[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39moffset\n",
      "File \u001b[1;32mc:\\Users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages\\kafka\\consumer\\fetcher.py:473\u001b[0m, in \u001b[0;36mFetcher._unpack_message_set\u001b[1;34m(self, tp, records)\u001b[0m\n\u001b[0;32m    469\u001b[0m value_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(record\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mif\u001b[39;00m record\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    470\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize(\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_deserializer\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    472\u001b[0m     tp\u001b[38;5;241m.\u001b[39mtopic, record\u001b[38;5;241m.\u001b[39mkey)\n\u001b[1;32m--> 473\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deserialize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue_deserializer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m headers \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mheaders\n\u001b[0;32m    477\u001b[0m header_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28mlen\u001b[39m(h_key\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mlen\u001b[39m(h_val) \u001b[38;5;28;01mif\u001b[39;00m h_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m h_key, h_val \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    479\u001b[0m     headers) \u001b[38;5;28;01mif\u001b[39;00m headers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ntlinh\\miniconda3\\envs\\kafka_lab\\lib\\site-packages\\kafka\\consumer\\fetcher.py:511\u001b[0m, in \u001b[0;36mFetcher._deserialize\u001b[1;34m(self, f, topic, bytes_)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, Deserializer):\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdeserialize(topic, bytes_)\n\u001b[1;32m--> 511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mCustomSerializer.deserialize\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeserialize\u001b[39m(data):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize bytes to object\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '{'."
     ]
    }
   ],
   "source": [
    "# Exercise 5: Custom Serializers and Deserializers\n",
    "print(\"ğŸ”§ Demonstrating custom serialization...\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "class CustomSerializer:\n",
    "    \"\"\"Custom serializer for complex objects\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def serialize(obj):\n",
    "        \"\"\"Serialize object to bytes\"\"\"\n",
    "        return pickle.dumps(obj)\n",
    "    \n",
    "    @staticmethod\n",
    "    def deserialize(data):\n",
    "        \"\"\"Deserialize bytes to object\"\"\"\n",
    "        return pickle.loads(data)\n",
    "\n",
    "# Create producer with custom serializer\n",
    "custom_producer = KafkaProducer(\n",
    "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    value_serializer=CustomSerializer.serialize,\n",
    "    key_serializer=lambda k: k.encode('utf-8') if k else None\n",
    ")\n",
    "\n",
    "# Create consumer with custom deserializer\n",
    "custom_consumer = KafkaConsumer(\n",
    "    TOPIC_NAME,\n",
    "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    group_id='custom-serialization-group',\n",
    "    auto_offset_reset='latest',\n",
    "    enable_auto_commit=True,\n",
    "    value_deserializer=CustomSerializer.deserialize,\n",
    "    key_deserializer=lambda x: x.decode('utf-8') if x else None,\n",
    "    consumer_timeout_ms=3000\n",
    ")\n",
    "\n",
    "print(\"âœ… Custom serializers/deserializers created!\")\n",
    "\n",
    "# Test custom serialization\n",
    "print(\"\\nğŸ§ª Testing custom serialization...\")\n",
    "\n",
    "# Create a complex object\n",
    "complex_data = {\n",
    "    \"symbol\": \"AAPL\",\n",
    "    \"price_data\": {\n",
    "        \"open\": 150.0,\n",
    "        \"high\": 155.0,\n",
    "        \"low\": 148.0,\n",
    "        \"close\": 152.0\n",
    "    },\n",
    "    \"indicators\": {\n",
    "        \"sma_20\": 150.5,\n",
    "        \"rsi\": 65.2,\n",
    "        \"macd\": 1.2\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"source\": \"market_data_api\",\n",
    "        \"version\": \"1.0\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send complex data\n",
    "future = custom_producer.send(TOPIC_NAME, key=\"AAPL\", value=complex_data)\n",
    "record_metadata = future.get(timeout=10)\n",
    "\n",
    "print(f\"ğŸ“Š Sent complex data to partition {record_metadata.partition}\")\n",
    "\n",
    "# Consume and verify\n",
    "print(\"\\nğŸ”„ Consuming complex data...\")\n",
    "for message in custom_consumer:\n",
    "    print(f\"ğŸ“Š Received complex data:\")\n",
    "    print(f\"   Symbol: {message.value['symbol']}\")\n",
    "    print(f\"   Price: ${message.value['price_data']['close']}\")\n",
    "    print(f\"   RSI: {message.value['indicators']['rsi']}\")\n",
    "    print(f\"   Source: {message.value['metadata']['source']}\")\n",
    "    break\n",
    "\n",
    "print(\"âœ… Custom serialization test completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›¡ï¸ Demonstrating error handling and retry logic...\n",
      "\n",
      "ğŸ§ª Testing error handling...\n",
      "âœ… Message sent successfully to partition 0\n",
      "âœ… Message sent successfully to partition 0\n",
      "âœ… Message sent successfully to partition 0\n",
      "âœ… Message sent successfully to partition 0\n",
      "âœ… Message sent successfully to partition 0\n",
      "ğŸ”„ Starting robust message consumption...\n",
      "\n",
      "ğŸ“ˆ Processing Summary:\n",
      "   Messages processed: 0\n",
      "   Errors handled: 0\n",
      "\n",
      "âœ… Error handling demonstration completed!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6: Error Handling and Retry Logic\n",
    "print(\"ğŸ›¡ï¸ Demonstrating error handling and retry logic...\")\n",
    "\n",
    "from functools import wraps\n",
    "\n",
    "class RetryableError(Exception):\n",
    "    \"\"\"Custom exception for retryable errors\"\"\"\n",
    "    pass\n",
    "\n",
    "class NonRetryableError(Exception):\n",
    "    \"\"\"Custom exception for non-retryable errors\"\"\"\n",
    "    pass\n",
    "\n",
    "def retry_with_backoff(max_retries=3, base_delay=1, max_delay=60):\n",
    "    \"\"\"Decorator for retry logic with exponential backoff\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            delay = base_delay\n",
    "            for attempt in range(max_retries + 1):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except RetryableError as e:\n",
    "                    if attempt == max_retries:\n",
    "                        print(f\"âŒ Max retries ({max_retries}) exceeded. Giving up.\")\n",
    "                        raise e\n",
    "                    \n",
    "                    print(f\"âš ï¸ Attempt {attempt + 1} failed: {e}\")\n",
    "                    print(f\"ğŸ”„ Retrying in {delay} seconds...\")\n",
    "                    time.sleep(delay)\n",
    "                    delay = min(delay * 2, max_delay)\n",
    "                    \n",
    "                except NonRetryableError as e:\n",
    "                    print(f\"âŒ Non-retryable error: {e}\")\n",
    "                    raise e\n",
    "                    \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Robust producer with error handling\n",
    "@retry_with_backoff(max_retries=3, base_delay=1)\n",
    "def send_message_with_retry(producer, topic, key, value):\n",
    "    \"\"\"Send message with retry logic\"\"\"\n",
    "    try:\n",
    "        future = producer.send(topic, key=key, value=value)\n",
    "        record_metadata = future.get(timeout=10)\n",
    "        print(f\"âœ… Message sent successfully to partition {record_metadata.partition}\")\n",
    "        return record_metadata\n",
    "    except Exception as e:\n",
    "        if \"timeout\" in str(e).lower():\n",
    "            raise RetryableError(f\"Timeout sending message: {e}\")\n",
    "        else:\n",
    "            raise NonRetryableError(f\"Non-retryable error: {e}\")\n",
    "\n",
    "# Create dedicated consumer for error handling demo\n",
    "error_handling_consumer = KafkaConsumer(\n",
    "    TOPIC_NAME,\n",
    "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    group_id='error-handling-demo-group',\n",
    "    auto_offset_reset='latest',\n",
    "    enable_auto_commit=True,\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
    "    key_deserializer=lambda x: x.decode('utf-8') if x else None,\n",
    "    consumer_timeout_ms=3000\n",
    ")\n",
    "\n",
    "# Robust consumer with error handling\n",
    "def consume_with_error_handling(consumer, max_messages=5):\n",
    "    \"\"\"Consume messages with error handling\"\"\"\n",
    "    messages_processed = 0\n",
    "    errors_handled = 0\n",
    "    \n",
    "    print(f\"ğŸ”„ Starting robust message consumption...\")\n",
    "    \n",
    "    try:\n",
    "        for message in consumer:\n",
    "            if messages_processed >= max_messages:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                data = message.value\n",
    "                \n",
    "                if not isinstance(data, dict) or 'symbol' not in data:\n",
    "                    print(f\"âš ï¸ Invalid message format, skipping...\")\n",
    "                    errors_handled += 1\n",
    "                    continue\n",
    "                \n",
    "                if random.random() < 0.3:  # 30% chance of error\n",
    "                    raise RetryableError(\"Simulated processing error\")\n",
    "                \n",
    "                print(f\"ğŸ“Š Processed message: {data['symbol']} - ${data['close']}\")\n",
    "                messages_processed += 1\n",
    "                \n",
    "            except RetryableError as e:\n",
    "                print(f\"âš ï¸ Retryable error processing message: {e}\")\n",
    "                errors_handled += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Non-retryable error: {e}\")\n",
    "                errors_handled += 1\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Consumer error: {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Processing Summary:\")\n",
    "    print(f\"   Messages processed: {messages_processed}\")\n",
    "    print(f\"   Errors handled: {errors_handled}\")\n",
    "    \n",
    "    return messages_processed, errors_handled\n",
    "\n",
    "# Test error handling\n",
    "print(\"\\nğŸ§ª Testing error handling...\")\n",
    "\n",
    "# Send some test data\n",
    "for i in range(5):\n",
    "    symbol = random.choice(STOCK_SYMBOLS)\n",
    "    ohlcv_data = data_generator.generate_ohlcv(symbol)\n",
    "    \n",
    "    try:\n",
    "        send_message_with_retry(producer, TOPIC_NAME, symbol, ohlcv_data)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to send message after retries: {e}\")\n",
    "\n",
    "# Test consumer error handling\n",
    "messages_processed, errors_handled = consume_with_error_handling(error_handling_consumer, 5)\n",
    "\n",
    "print(\"\\nâœ… Error handling demonstration completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Analyzing Kafka performance...\n",
      "âœ… High-performance producer and consumer created!\n",
      "\n",
      "ğŸš€ Testing producer throughput...\n",
      "ğŸ“Š Producer Performance:\n",
      "   Messages sent: 100\n",
      "   Time taken: 6.23 seconds\n",
      "   Throughput: 16.04 messages/second\n",
      "\n",
      "ğŸ”„ Testing consumer throughput...\n",
      "ğŸ“Š Consumer Performance:\n",
      "   Messages consumed: 0\n",
      "   Time taken: 3.63 seconds\n",
      "   Throughput: 0.00 messages/second\n",
      "\n",
      "ğŸ“ˆ Performance Summary:\n",
      "   avg_producer_throughput: 16.04\n",
      "   max_producer_throughput: 16.04\n",
      "   avg_consumer_throughput: 0.00\n",
      "   max_consumer_throughput: 0.00\n",
      "   avg_message_size_bytes: 166.29\n",
      "   total_messages: 100\n",
      "   partition_distribution: {0: 100}\n",
      "\n",
      "âœ… Performance analysis completed!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 7: Performance Analysis and Monitoring\n",
    "print(\"ğŸ“Š Analyzing Kafka performance...\")\n",
    "\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Monitor Kafka performance metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'producer_throughput': [],\n",
    "            'consumer_throughput': [],\n",
    "            'latency': [],\n",
    "            'message_sizes': [],\n",
    "            'partition_distribution': defaultdict(int)\n",
    "        }\n",
    "    \n",
    "    def record_producer_metric(self, messages_sent, time_taken):\n",
    "        \"\"\"Record producer performance metric\"\"\"\n",
    "        throughput = messages_sent / time_taken if time_taken > 0 else 0\n",
    "        self.metrics['producer_throughput'].append(throughput)\n",
    "    \n",
    "    def record_consumer_metric(self, messages_consumed, time_taken):\n",
    "        \"\"\"Record consumer performance metric\"\"\"\n",
    "        throughput = messages_consumed / time_taken if time_taken > 0 else 0\n",
    "        self.metrics['consumer_throughput'].append(throughput)\n",
    "    \n",
    "    def record_latency(self, latency_ms):\n",
    "        \"\"\"Record message latency\"\"\"\n",
    "        self.metrics['latency'].append(latency_ms)\n",
    "    \n",
    "    def record_message_size(self, size_bytes):\n",
    "        \"\"\"Record message size\"\"\"\n",
    "        self.metrics['message_sizes'].append(size_bytes)\n",
    "    \n",
    "    def record_partition(self, partition):\n",
    "        \"\"\"Record partition distribution\"\"\"\n",
    "        self.metrics['partition_distribution'][partition] += 1\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get performance summary\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        if self.metrics['producer_throughput']:\n",
    "            summary['avg_producer_throughput'] = statistics.mean(self.metrics['producer_throughput'])\n",
    "            summary['max_producer_throughput'] = max(self.metrics['producer_throughput'])\n",
    "        \n",
    "        if self.metrics['consumer_throughput']:\n",
    "            summary['avg_consumer_throughput'] = statistics.mean(self.metrics['consumer_throughput'])\n",
    "            summary['max_consumer_throughput'] = max(self.metrics['consumer_throughput'])\n",
    "        \n",
    "        if self.metrics['latency']:\n",
    "            summary['avg_latency_ms'] = statistics.mean(self.metrics['latency'])\n",
    "            summary['max_latency_ms'] = max(self.metrics['latency'])\n",
    "            summary['min_latency_ms'] = min(self.metrics['latency'])\n",
    "        \n",
    "        if self.metrics['message_sizes']:\n",
    "            summary['avg_message_size_bytes'] = statistics.mean(self.metrics['message_sizes'])\n",
    "            summary['total_messages'] = len(self.metrics['message_sizes'])\n",
    "        \n",
    "        summary['partition_distribution'] = dict(self.metrics['partition_distribution'])\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Initialize performance monitor\n",
    "perf_monitor = PerformanceMonitor()\n",
    "\n",
    "# High-performance producer with gzip compression (compatible)\n",
    "high_perf_producer = KafkaProducer(\n",
    "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "    key_serializer=lambda k: k.encode('utf-8') if k else None,\n",
    "    batch_size=32768,\n",
    "    linger_ms=50,\n",
    "    compression_type='gzip',  # Use gzip (always available)\n",
    "    acks=1,\n",
    "    retries=0\n",
    ")\n",
    "\n",
    "# High-performance consumer\n",
    "high_perf_consumer = KafkaConsumer(\n",
    "    TOPIC_NAME,\n",
    "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    group_id='performance-test-group',\n",
    "    auto_offset_reset='latest',\n",
    "    enable_auto_commit=True,\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
    "    key_deserializer=lambda x: x.decode('utf-8') if x else None,\n",
    "    consumer_timeout_ms=2000,\n",
    "    fetch_min_bytes=1024,\n",
    "    fetch_max_wait_ms=500\n",
    ")\n",
    "\n",
    "print(\"âœ… High-performance producer and consumer created!\")\n",
    "\n",
    "# Performance test: Producer throughput\n",
    "print(\"\\nğŸš€ Testing producer throughput...\")\n",
    "num_messages = 100\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(num_messages):\n",
    "    symbol = random.choice(STOCK_SYMBOLS)\n",
    "    ohlcv_data = data_generator.generate_ohlcv(symbol)\n",
    "    \n",
    "    message_size = len(json.dumps(ohlcv_data).encode('utf-8'))\n",
    "    perf_monitor.record_message_size(message_size)\n",
    "    \n",
    "    future = high_perf_producer.send(TOPIC_NAME, key=symbol, value=ohlcv_data)\n",
    "    record_metadata = future.get(timeout=5)\n",
    "    \n",
    "    perf_monitor.record_partition(record_metadata.partition)\n",
    "\n",
    "high_perf_producer.flush()\n",
    "end_time = time.time()\n",
    "producer_time = end_time - start_time\n",
    "\n",
    "perf_monitor.record_producer_metric(num_messages, producer_time)\n",
    "\n",
    "print(f\"ğŸ“Š Producer Performance:\")\n",
    "print(f\"   Messages sent: {num_messages}\")\n",
    "print(f\"   Time taken: {producer_time:.2f} seconds\")\n",
    "print(f\"   Throughput: {num_messages/producer_time:.2f} messages/second\")\n",
    "\n",
    "# Performance test: Consumer throughput\n",
    "print(\"\\nğŸ”„ Testing consumer throughput...\")\n",
    "start_time = time.time()\n",
    "messages_consumed = 0\n",
    "\n",
    "for message in high_perf_consumer:\n",
    "    if messages_consumed >= num_messages:\n",
    "        break\n",
    "    \n",
    "    data = message.value\n",
    "    latency_ms = (time.time() * 1000) - message.timestamp\n",
    "    perf_monitor.record_latency(latency_ms)\n",
    "    messages_consumed += 1\n",
    "\n",
    "end_time = time.time()\n",
    "consumer_time = end_time - start_time\n",
    "\n",
    "perf_monitor.record_consumer_metric(messages_consumed, consumer_time)\n",
    "\n",
    "print(f\"ğŸ“Š Consumer Performance:\")\n",
    "print(f\"   Messages consumed: {messages_consumed}\")\n",
    "print(f\"   Time taken: {consumer_time:.2f} seconds\")\n",
    "print(f\"   Throughput: {messages_consumed/consumer_time:.2f} messages/second\")\n",
    "\n",
    "# Get performance summary\n",
    "summary = perf_monitor.get_summary()\n",
    "print(f\"\\nğŸ“ˆ Performance Summary:\")\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nâœ… Performance analysis completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ Cleaning up resources and reviewing best practices...\n",
      "ğŸ”§ Closing Kafka resources...\n",
      "âœ… Closed Producer\n",
      "âœ… Closed Custom Producer\n",
      "âœ… Closed High Performance Producer\n",
      "âœ… Closed Consumer\n",
      "âœ… Closed Custom Consumer\n",
      "âœ… Closed High Performance Consumer\n",
      "âœ… Closed Error Handling Consumer\n",
      "\n",
      "âœ… All resources closed successfully!\n",
      "\n",
      "ğŸ“š Kafka Best Practices Summary:\n",
      "\n",
      "ğŸ”§ Producer Best Practices:\n",
      "   1. Use appropriate acknowledgment levels (acks)\n",
      "   2. Implement retry logic with exponential backoff\n",
      "   3. Use batching for better throughput\n",
      "   4. Choose appropriate compression (gzip, snappy, lz4)\n",
      "   5. Always call flush() before closing\n",
      "   6. Use meaningful keys for partitioning\n",
      "\n",
      "ğŸ”„ Consumer Best Practices:\n",
      "   1. Use consumer groups for scalability\n",
      "   2. Implement proper error handling\n",
      "   3. Monitor consumer lag\n",
      "   4. Use appropriate auto_offset_reset\n",
      "   5. Implement idempotent processing\n",
      "   6. Handle rebalancing gracefully\n",
      "\n",
      "ğŸ“Š Performance Best Practices:\n",
      "   1. Tune batch sizes based on use case\n",
      "   2. Use compression for large messages\n",
      "   3. Monitor throughput and latency\n",
      "   4. Optimize partition count\n",
      "   5. Use appropriate replication factor\n",
      "   6. Implement proper monitoring\n",
      "\n",
      "ğŸ›¡ï¸ Error Handling Best Practices:\n",
      "   1. Implement retry logic with backoff\n",
      "   2. Use dead letter queues for failed messages\n",
      "   3. Implement circuit breaker patterns\n",
      "   4. Log errors appropriately\n",
      "   5. Monitor error rates\n",
      "   6. Implement graceful degradation\n",
      "\n",
      "ğŸ“ˆ Monitoring Best Practices:\n",
      "   1. Monitor producer/consumer throughput\n",
      "   2. Track message latency\n",
      "   3. Monitor consumer lag\n",
      "   4. Set up alerts for failures\n",
      "   5. Use proper logging levels\n",
      "   6. Implement health checks\n",
      "\n",
      "ğŸ¯ Lab 1 Summary - What We Learned:\n",
      "âœ… Kafka Fundamentals: Topics, Partitions, Producers, Consumers\n",
      "âœ… Message Serialization: JSON and custom serializers\n",
      "âœ… Error Handling: Retry logic and exception handling\n",
      "âœ… Performance Analysis: Throughput and latency measurement\n",
      "âœ… Data Visualization: Real-time charts and dashboards\n",
      "âœ… Best Practices: Resource management and optimization\n",
      "\n",
      "ğŸš€ Next Steps:\n",
      "- Lab 2: Consumer Groups and Load Balancing\n",
      "- Lab 3: Partitioning Strategies\n",
      "- Lab 4: Offset Management\n",
      "- Lab 5: Real-time Analytics\n",
      "\n",
      "ğŸ’¡ Key Takeaways:\n",
      "1. Kafka enables high-throughput, low-latency messaging\n",
      "2. Proper configuration is crucial for performance\n",
      "3. Error handling and monitoring are essential\n",
      "4. Visualization helps understand data patterns\n",
      "5. Resource cleanup prevents memory leaks\n",
      "\n",
      "ğŸ‰ Lab 1 completed successfully!\n",
      "Ready to move on to Lab 2: Consumer Groups!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 9: Best Practices and Cleanup\n",
    "print(\"ğŸ§¹ Cleaning up resources and reviewing best practices...\")\n",
    "\n",
    "def cleanup_resources():\n",
    "    \"\"\"Properly close all Kafka resources\"\"\"\n",
    "    resources_to_close = [\n",
    "        ('Producer', producer),\n",
    "        ('Custom Producer', custom_producer),\n",
    "        ('High Performance Producer', high_perf_producer),\n",
    "        ('Consumer', consumer),\n",
    "        ('Custom Consumer', custom_consumer),\n",
    "        ('High Performance Consumer', high_perf_consumer),\n",
    "        ('Error Handling Consumer', error_handling_consumer)    \n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ”§ Closing Kafka resources...\")\n",
    "    \n",
    "    for resource_name, resource in resources_to_close:\n",
    "        try:\n",
    "            if resource:\n",
    "                resource.close()\n",
    "                print(f\"âœ… Closed {resource_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error closing {resource_name}: {e}\")\n",
    "    \n",
    "    print(\"\\nâœ… All resources closed successfully!\")\n",
    "\n",
    "# Cleanup all resources\n",
    "cleanup_resources()\n",
    "\n",
    "# Best Practices Summary\n",
    "print(\"\\nğŸ“š Kafka Best Practices Summary:\")\n",
    "print(\"\\nğŸ”§ Producer Best Practices:\")\n",
    "print(\"   1. Use appropriate acknowledgment levels (acks)\")\n",
    "print(\"   2. Implement retry logic with exponential backoff\")\n",
    "print(\"   3. Use batching for better throughput\")\n",
    "print(\"   4. Choose appropriate compression (gzip, snappy, lz4)\")\n",
    "print(\"   5. Always call flush() before closing\")\n",
    "print(\"   6. Use meaningful keys for partitioning\")\n",
    "\n",
    "print(\"\\nğŸ”„ Consumer Best Practices:\")\n",
    "print(\"   1. Use consumer groups for scalability\")\n",
    "print(\"   2. Implement proper error handling\")\n",
    "print(\"   3. Monitor consumer lag\")\n",
    "print(\"   4. Use appropriate auto_offset_reset\")\n",
    "print(\"   5. Implement idempotent processing\")\n",
    "print(\"   6. Handle rebalancing gracefully\")\n",
    "\n",
    "print(\"\\nğŸ“Š Performance Best Practices:\")\n",
    "print(\"   1. Tune batch sizes based on use case\")\n",
    "print(\"   2. Use compression for large messages\")\n",
    "print(\"   3. Monitor throughput and latency\")\n",
    "print(\"   4. Optimize partition count\")\n",
    "print(\"   5. Use appropriate replication factor\")\n",
    "print(\"   6. Implement proper monitoring\")\n",
    "\n",
    "print(\"\\nğŸ›¡ï¸ Error Handling Best Practices:\")\n",
    "print(\"   1. Implement retry logic with backoff\")\n",
    "print(\"   2. Use dead letter queues for failed messages\")\n",
    "print(\"   3. Implement circuit breaker patterns\")\n",
    "print(\"   4. Log errors appropriately\")\n",
    "print(\"   5. Monitor error rates\")\n",
    "print(\"   6. Implement graceful degradation\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ Monitoring Best Practices:\")\n",
    "print(\"   1. Monitor producer/consumer throughput\")\n",
    "print(\"   2. Track message latency\")\n",
    "print(\"   3. Monitor consumer lag\")\n",
    "print(\"   4. Set up alerts for failures\")\n",
    "print(\"   5. Use proper logging levels\")\n",
    "print(\"   6. Implement health checks\")\n",
    "\n",
    "# Lab Summary\n",
    "print(\"\\nğŸ¯ Lab 1 Summary - What We Learned:\")\n",
    "print(\"âœ… Kafka Fundamentals: Topics, Partitions, Producers, Consumers\")\n",
    "print(\"âœ… Message Serialization: JSON and custom serializers\")\n",
    "print(\"âœ… Error Handling: Retry logic and exception handling\")\n",
    "print(\"âœ… Performance Analysis: Throughput and latency measurement\")\n",
    "print(\"âœ… Data Visualization: Real-time charts and dashboards\")\n",
    "print(\"âœ… Best Practices: Resource management and optimization\")\n",
    "\n",
    "print(\"\\nğŸš€ Next Steps:\")\n",
    "print(\"- Lab 2: Consumer Groups and Load Balancing\")\n",
    "print(\"- Lab 3: Partitioning Strategies\")\n",
    "print(\"- Lab 4: Offset Management\")\n",
    "print(\"- Lab 5: Real-time Analytics\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Takeaways:\")\n",
    "print(\"1. Kafka enables high-throughput, low-latency messaging\")\n",
    "print(\"2. Proper configuration is crucial for performance\")\n",
    "print(\"3. Error handling and monitoring are essential\")\n",
    "print(\"4. Visualization helps understand data patterns\")\n",
    "print(\"5. Resource cleanup prevents memory leaks\")\n",
    "\n",
    "print(\"\\nğŸ‰ Lab 1 completed successfully!\")\n",
    "print(\"Ready to move on to Lab 2: Consumer Groups!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
